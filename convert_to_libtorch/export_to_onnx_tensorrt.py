import os
import sys
import torch
import tensorrt as trt
import onnx
import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit  # è‡ªå‹•åˆå§‹åŒ– CUDA

# æª”æ¡ˆè·¯å¾‘
input_path = "/circ330/forgithub/VisualFusion_libtorch/Onnx/model/onnxModel/SemLA_onnx_320x240_opencv_compatible.onnx"
output_path = "/circ330/forgithub/VisualFusion_libtorch/convert_to_libtorch/converModel/SemLA_tensorrt.engine"

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
os.makedirs(os.path.dirname(output_path), exist_ok=True)

def setup_cuda_environment():
    """è¨­ç½® CUDA ç’°å¢ƒä¸¦é¸æ“‡ GPU"""
    print("=== è¨­ç½® CUDA ç’°å¢ƒ ===")
    
    # æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("éŒ¯èª¤: CUDA ä¸å¯ç”¨")
        return False
    
    # ç²å–å¯ç”¨çš„ GPU
    num_gpus = torch.cuda.device_count()
    print(f"å¯ç”¨çš„ GPU æ•¸é‡: {num_gpus}")
    
    # é¡¯ç¤ºæ‰€æœ‰ GPU ä¿¡æ¯
    for i in range(num_gpus):
        gpu_props = torch.cuda.get_device_properties(i)
        print(f"GPU {i}: {gpu_props.name}")
        print(f"  è¨˜æ†¶é«”: {gpu_props.total_memory / 1024**3:.1f} GB")
        print(f"  è¨ˆç®—èƒ½åŠ›: {gpu_props.major}.{gpu_props.minor}")
    
    # è¨­ç½® GPU è¨­å‚™
    cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '0')
    gpu_id = int(cuda_visible_devices.split(',')[0])
    
    if gpu_id >= num_gpus:
        gpu_id = 0
        print(f"è­¦å‘Š: æŒ‡å®šçš„ GPU {gpu_id} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ GPU 0")
    
    torch.cuda.set_device(gpu_id)
    print(f"ä½¿ç”¨ GPU {gpu_id}: {torch.cuda.get_device_properties(gpu_id).name}")
    
    # æ¸…ç† GPU è¨˜æ†¶é«”
    torch.cuda.empty_cache()
    
    return True

# TensorRT Logger è¨­ç½®ç‚ºè¼ƒä½çš„æ—¥å¿—ç´šåˆ¥ä»¥æ¸›å°‘è­¦å‘Š
TRT_LOGGER = trt.Logger(trt.Logger.ERROR)

def check_onnx_model(onnx_path):
    """æª¢æŸ¥ ONNX æ¨¡å‹çš„åŸºæœ¬ä¿¡æ¯"""
    print("=== æª¢æŸ¥ ONNX æ¨¡å‹ ===")
    model = onnx.load(onnx_path)
    print(f"ONNX æ¨¡å‹ç‰ˆæœ¬: {model.ir_version}")
    print(f"ç”Ÿç”¢è€…åç¨±: {model.producer_name}")
    print(f"ç”Ÿç”¢è€…ç‰ˆæœ¬: {model.producer_version}")
    
    # æª¢æŸ¥è¼¸å…¥
    print("\nè¼¸å…¥å¼µé‡:")
    for input_tensor in model.graph.input:
        print(f"  åç¨±: {input_tensor.name}")
        shape = [dim.dim_value for dim in input_tensor.type.tensor_type.shape.dim]
        print(f"  å½¢ç‹€: {shape}")
        print(f"  é¡å‹: {input_tensor.type.tensor_type.elem_type}")
    
    # æª¢æŸ¥è¼¸å‡º
    print("\nè¼¸å‡ºå¼µé‡:")
    for output_tensor in model.graph.output:
        print(f"  åç¨±: {output_tensor.name}")
        shape = [dim.dim_value if dim.dim_value > 0 else -1 for dim in output_tensor.type.tensor_type.shape.dim]
        print(f"  å½¢ç‹€: {shape}")
        print(f"  é¡å‹: {output_tensor.type.tensor_type.elem_type}")
    
    return True

def onnx2trt(onnx_path, engine_path):
    """å°‡ ONNX æ¨¡å‹è½‰æ›ç‚º TensorRT Engine"""
    print(f"\n=== é–‹å§‹è½‰æ› ONNX åˆ° TensorRT ===")
    print(f"è¼¸å…¥æª”æ¡ˆ: {onnx_path}")
    print(f"è¼¸å‡ºæª”æ¡ˆ: {engine_path}")
    
    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(onnx_path):
        print(f"éŒ¯èª¤: ONNX æª”æ¡ˆä¸å­˜åœ¨: {onnx_path}")
        return False
    
    try:
        # å‰µå»º builder
        builder = trt.Builder(TRT_LOGGER)
        
        # è¨­ç½®æ›´ä¿å®ˆçš„é…ç½®ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ
        if hasattr(builder, 'max_batch_size'):
            builder.max_batch_size = 1
        
        # å‰µå»ºç¶²è·¯ï¼Œä½¿ç”¨é¡¯å¼æ‰¹æ¬¡å¤§å°
        network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
        network = builder.create_network(network_flags)
        
        # å‰µå»º ONNX è§£æå™¨
        parser = trt.OnnxParser(network, TRT_LOGGER)
        
        # è§£æ ONNX æª”æ¡ˆ
        print("è§£æ ONNX æª”æ¡ˆ...")
        with open(onnx_path, 'rb') as model:
            model_data = model.read()
            success = parser.parse(model_data)
            
        if not success:
            print("éŒ¯èª¤: ONNX è§£æå¤±æ•—")
            for i in range(parser.num_errors):
                error = parser.get_error(i)
                print(f"  éŒ¯èª¤ {i}: {error}")
            return False
        
        print("ONNX è§£ææˆåŠŸ!")
        
        # å‰µå»ºå»ºæ§‹å™¨é…ç½®
        config = builder.create_builder_config()
        
        # è¨­å®šè¼ƒå°çš„å·¥ä½œç©ºé–“å¤§å°ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ (512MB)
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 512 * 1024 * 1024)
        
        # æš«æ™‚ä¸å•Ÿç”¨ FP16ï¼Œå…ˆç¢ºä¿åŸºæœ¬è½‰æ›æˆåŠŸ
        print("ä½¿ç”¨ FP32 ç²¾åº¦é€²è¡Œè½‰æ›")
        
        # å‰µå»ºå„ªåŒ–è¨­å®šæª”
        profile = builder.create_optimization_profile()
        
        # è¨­å®šè¼¸å…¥å¼µé‡çš„ç¶­åº¦ï¼ˆå›ºå®šæ‰¹æ¬¡å¤§å°ï¼‰
        input_shape = (1, 1, 240, 320)
        min_shape = input_shape
        opt_shape = input_shape  
        max_shape = input_shape
        
        profile.set_shape("vi_img", min_shape, opt_shape, max_shape)
        profile.set_shape("ir_img", min_shape, opt_shape, max_shape)
        
        config.add_optimization_profile(profile)
        
        print("å»ºæ§‹ TensorRT å¼•æ“...")
        print("é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜æ™‚é–“...")
        
        # å»ºæ§‹å¼•æ“ - ä½¿ç”¨è¼ƒæ–°çš„ API
        plan = builder.build_serialized_network(network, config)
        
        if plan is None:
            print("éŒ¯èª¤: TensorRT å¼•æ“å»ºæ§‹å¤±æ•—")
            return False
        
        # å„²å­˜å¼•æ“
        print(f"å„²å­˜ TensorRT å¼•æ“åˆ°: {engine_path}")
        with open(engine_path, 'wb') as f:
            f.write(plan)
        
        print("âœ… TensorRT è½‰æ›æˆåŠŸ!")
        
        # é©—è­‰ç”Ÿæˆçš„å¼•æ“
        return verify_engine(engine_path)
        
    except Exception as e:
        print(f"éŒ¯èª¤: TensorRT è½‰æ›å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def verify_engine(engine_path):
    """é©—è­‰ç”Ÿæˆçš„ TensorRT å¼•æ“"""
    print(f"\n=== é©—è­‰ TensorRT å¼•æ“ ===")
    
    try:
        # è¼‰å…¥å¼•æ“
        runtime = trt.Runtime(TRT_LOGGER)
        with open(engine_path, 'rb') as f:
            engine_data = f.read()
        engine = runtime.deserialize_cuda_engine(engine_data)
        
        if engine is None:
            print("éŒ¯èª¤: ç„¡æ³•è¼‰å…¥ TensorRT å¼•æ“")
            return False
        
        print("âœ… TensorRT å¼•æ“è¼‰å…¥æˆåŠŸ!")
        
        # é¡¯ç¤ºå¼•æ“ä¿¡æ¯
        if hasattr(engine, 'name'):
            print(f"å¼•æ“åç¨±: {engine.name}")
        if hasattr(engine, 'max_batch_size'):
            print(f"æœ€å¤§æ‰¹æ¬¡å¤§å°: {engine.max_batch_size}")
        if hasattr(engine, 'device_memory_size'):
            print(f"å·¥ä½œç©ºé–“å¤§å°: {engine.device_memory_size} bytes")
        print(f"ç¶å®šæ•¸é‡: {engine.num_bindings}")
        
        # é¡¯ç¤ºè¼¸å…¥/è¼¸å‡ºä¿¡æ¯
        for i in range(engine.num_bindings):
            binding_name = engine.get_binding_name(i)
            binding_shape = engine.get_binding_shape(i)
            binding_dtype = engine.get_binding_dtype(i)
            
            if engine.binding_is_input(i):
                print(f"è¼¸å…¥ {i}: {binding_name}, å½¢ç‹€: {binding_shape}, é¡å‹: {binding_dtype}")
            else:
                print(f"è¼¸å‡º {i}: {binding_name}, å½¢ç‹€: {binding_shape}, é¡å‹: {binding_dtype}")
        
        return True
        
    except Exception as e:
        print(f"éŒ¯èª¤: å¼•æ“é©—è­‰å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("=== SemLA ONNX åˆ° TensorRT è½‰æ›å·¥å…· ===")
    
    # è¨­ç½® CUDA ç’°å¢ƒ
    if not setup_cuda_environment():
        print("âŒ CUDA ç’°å¢ƒè¨­ç½®å¤±æ•—!")
        sys.exit(1)
    
    # æª¢æŸ¥ ONNX æ¨¡å‹
    if check_onnx_model(input_path):
        # è½‰æ›åˆ° TensorRT
        success = onnx2trt(input_path, output_path)
        
        if success:
            print(f"\nğŸ‰ è½‰æ›å®Œæˆ! TensorRT å¼•æ“å·²å„²å­˜åˆ°: {output_path}")
        else:
            print("\nâŒ è½‰æ›å¤±æ•—!")
    else:
        print("âŒ ONNX æ¨¡å‹æª¢æŸ¥å¤±æ•—!")
