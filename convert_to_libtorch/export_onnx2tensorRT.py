#!/usr/bin/env python3
"""
ONNX to TensorRT Conversion Script
å°‡å›ºå®šå½¢ç‹€çš„ ONNX æ¨¡å‹è½‰æ›ç‚º TensorRT å¼•æ“

åŸºæ–¼:
- /circ330/forgithub/VisualFusion_libtorch/convert_to_libtorch/model_jit æ¨¡å‹
- /circ330/forgithub/VisualFusion_libtorch/convert_to_libtorch/export_to_onnx_fixed.py ç”Ÿæˆçš„å›ºå®šå½¢ç‹€ ONNX æ¨¡å‹

Usage:
    python export_onnx2tensorRT.py
"""

import tensorrt as trt
import numpy as np
import os
import argparse
from pathlib import Path

class ONNXToTensorRTConverter:
    def __init__(self):
        # å‰µå»º TensorRT loggerï¼Œä½¿ç”¨ WARNING ç­‰ç´šé¿å…éå¤šè¼¸å‡º
        self.logger = trt.Logger(trt.Logger.WARNING)

    def convert_onnx_to_trt(self, onnx_path, trt_path, max_batch_size=1, fp16_mode=True, max_workspace_size=1<<30):
        """
        å°‡ ONNX æ¨¡å‹è½‰æ›ç‚º TensorRT å¼•æ“

        Args:
            onnx_path: è¼¸å…¥ ONNX æ¨¡å‹è·¯å¾‘
            trt_path: è¼¸å‡º TensorRT å¼•æ“è·¯å¾‘
            max_batch_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
            fp16_mode: æ˜¯å¦å•Ÿç”¨ FP16 ç²¾åº¦
            max_workspace_size: æœ€å¤§å·¥ä½œç©ºé–“å¤§å° (bytes)
        """
        print(f"ğŸ”„ Converting ONNX to TensorRT...")
        print(f"ğŸ“ Input ONNX: {onnx_path}")
        print(f"ğŸ’¾ Output TRT: {trt_path}")

        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if not os.path.exists(onnx_path):
            raise FileNotFoundError(f"ONNX file not found: {onnx_path}")

        # å‰µå»º builder å’Œ network
        builder = trt.Builder(self.logger)
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, self.logger)
        
        # è§£æ ONNX æ¨¡å‹
        print("ğŸ“– Parsing ONNX model...")
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                print("âŒ Failed to parse ONNX model")
                for error in range(parser.num_errors):
                    print(f"  Error {error}: {parser.get_error(error)}")
                return False

        print("âœ… ONNX model parsed successfully")

        # é¡¯ç¤ºç¶²è·¯ä¿¡æ¯
        print(f"ğŸ“Š Network information:")
        print(f"  ğŸ”¢ Number of inputs: {network.num_inputs}")
        print(f"  ğŸ”¢ Number of outputs: {network.num_outputs}")

        for i in range(network.num_inputs):
            tensor = network.get_input(i)
            print(f"  ğŸ“¥ Input {i}: {tensor.name}")
            print(f"      Shape: {tensor.shape}")
            print(f"      Dtype: {tensor.dtype}")

        for i in range(network.num_outputs):
            tensor = network.get_output(i)
            print(f"  ğŸ“¤ Output {i}: {tensor.name}")
            print(f"      Shape: {tensor.shape}")
            print(f"      Dtype: {tensor.dtype}")

        # å‰µå»ºå»ºæ§‹é…ç½®
        config = builder.create_builder_config()
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_workspace_size)

        # å•Ÿç”¨ FP16 ç²¾åº¦ï¼ˆå¦‚æœæ”¯æŒä¸”è«‹æ±‚ï¼‰
        if fp16_mode and builder.platform_has_fast_fp16:
            print("ğŸš€ Enabling FP16 precision for faster inference")
            config.set_flag(trt.BuilderFlag.FP16)
        else:
            print("âš¡ Using FP32 precision for stability")

        # è¨­å®šå„ªåŒ–é…ç½®æ–‡ä»¶ï¼ˆé‡å°å›ºå®šè¼¸å…¥å½¢ç‹€ï¼‰
        profile = builder.create_optimization_profile()
        

        # åŸºæ–¼ SemLA æ¨¡å‹çš„å›ºå®šå½¢ç‹€è¨­å®š
        # è¼¸å…¥: vi_img (1, 1, 240, 320), ir_img (1, 1, 240, 320)
        input_shapes = [
            (1, 1, 240, 320),  # vi_img
            (1, 1, 240, 320)   # ir_img
        ]

        for i in range(network.num_inputs):
            tensor = network.get_input(i)
            shape = input_shapes[i]
            print(f"âš™ï¸  Setting optimization profile for {tensor.name}: {shape}")
            # ç¢ºä¿æ‰€æœ‰ min, opt, max éƒ½æ˜¯å›ºå®šå½¢ç‹€ï¼Œé¿å…å‹•æ…‹å°ºå¯¸å•é¡Œ
            profile.set_shape(tensor.name, shape, shape, shape)

        # ç§»é™¤ is_valid() æª¢æŸ¥ï¼Œå› ç‚º TensorRT 10.x ç‰ˆæœ¬æ²’æœ‰é€™å€‹æ–¹æ³•
        config.add_optimization_profile(profile)

        # å»ºæ§‹ TensorRT å¼•æ“
        print("ğŸ”„ Building TensorRT engine... (this may take several minutes)")
        print("   ğŸ’­ Please be patient, optimizing network for your hardware...")

        serialized_engine = builder.build_serialized_network(network, config)

        if serialized_engine is None:
            print("âŒ Failed to build TensorRT engine")
            return False

        # å„²å­˜å¼•æ“åˆ°æ–‡ä»¶
        os.makedirs(os.path.dirname(trt_path), exist_ok=True)
        with open(trt_path, 'wb') as f:
            f.write(serialized_engine)

        print(f"âœ… TensorRT engine saved successfully!")
        print(f"ğŸ’¾ Engine file: {trt_path}")
        print(f"ğŸ“ File size: {os.path.getsize(trt_path) / (1024*1024):.2f} MB")

        # é©—è­‰å¼•æ“
        return self.validate_engine(trt_path)

    def validate_engine(self, trt_path):
        """é©—è­‰å‰µå»ºçš„ TensorRT å¼•æ“"""
        print("ğŸ” Validating TensorRT engine...")

        try:
            # è¼‰å…¥å¼•æ“
            runtime = trt.Runtime(self.logger)
            with open(trt_path, 'rb') as f:
                engine_data = f.read()

            engine = runtime.deserialize_cuda_engine(engine_data)
            if not engine:
                print("âŒ Failed to load created engine")
                return False

            context = engine.create_execution_context()
            if not context:
                print("âŒ Failed to create execution context")
                return False

            print("ğŸ“‹ Engine validation results:")
            print(f"  ğŸ”¢ Number of bindings: {engine.num_bindings}")

            for i in range(engine.num_bindings):
                name = engine.get_binding_name(i)
                shape = engine.get_binding_shape(i)
                dtype = engine.get_binding_dtype(i)
                is_input = engine.binding_is_input(i)
                binding_type = "Input" if is_input else "Output"

                print(f"  {'ğŸ“¥' if is_input else 'ğŸ“¤'} {binding_type} {i}: {name}")
                print(f"      Shape: {shape}")
                print(f"      Dtype: {dtype}")

            print("âœ… Engine validation passed!")
            return True

        except Exception as e:
            print(f"âŒ Engine validation failed: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description='Convert ONNX model to TensorRT engine')
    parser.add_argument('--onnx', type=str,
                       default='/circ330/forgithub/VisualFusion_libtorch/Onnx/model/onnxModel/SemLA_onnx_320x240_fixed1200pts.onnx',
                       help='Path to input ONNX model')
    parser.add_argument('--trt', type=str,
                       default='/circ330/forgithub/VisualFusion_libtorch/tensorRT/model/trtModel/trt_1200kps.engine',
                       help='Path to output TensorRT engine')
    parser.add_argument('--fp16', action='store_true', default=False,
                       help='Enable FP16 precision (default: True)')
    parser.add_argument('--workspace-size', type=int, default=1024,
                       help='Max workspace size in MB (default: 1024)')

    args = parser.parse_args()

    print("ğŸ¯ ONNX to TensorRT Conversion Tool")
    print("=" * 50)
    print("ğŸ“‹ Configuration:")
    print(f"  ğŸ“ ONNX model: {args.onnx}")
    print(f"  ğŸ’¾ TRT engine: {args.trt}")
    print(f"  ğŸš€ FP16 mode: {args.fp16}")
    print(f"  ğŸ’¾ Workspace: {args.workspace_size} MB")
    print("=" * 50)

    # å‰µå»ºè½‰æ›å™¨ä¸¦åŸ·è¡Œè½‰æ›
    converter = ONNXToTensorRTConverter()

    success = converter.convert_onnx_to_trt(
        onnx_path=args.onnx,
        trt_path=args.trt,
        fp16_mode=args.fp16,
        max_workspace_size=args.workspace_size * 1024 * 1024  # Convert MB to bytes
    )

    if success:
        print("\nğŸ‰ Conversion completed successfully!")
        print(f"ğŸ“Œ You can now use the TensorRT engine: {args.trt}")
        print("ğŸ”§ Update your configuration files to use this new engine.")
    else:
        print("\nğŸ’¥ Conversion failed!")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())
